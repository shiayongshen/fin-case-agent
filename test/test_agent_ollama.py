#!/usr/bin/env python3
"""
æ¸¬è©¦è…³æœ¬ï¼šé©—è­‰ Ollama é…ç½®æ˜¯å¦æ­£ç¢ºæ‡‰ç”¨åˆ° Agent System
"""

import os
import sys
from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv(".env.ollama.example")

def test_environment_setup():
    """æ¸¬è©¦ç’°å¢ƒè®Šæ•¸è¨­å®š"""
    print("=" * 70)
    print("æ¸¬è©¦ 1: ç’°å¢ƒè®Šæ•¸è¨­å®š")
    print("=" * 70)
    
    use_ollama = os.getenv("USE_OLLAMA", "false").lower() == "true"
    ollama_model = os.getenv("OLLAMA_MODEL", "mistral")
    ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    print(f"\nâœ“ USE_OLLAMA: {use_ollama}")
    print(f"âœ“ OLLAMA_MODEL: {ollama_model}")
    print(f"âœ“ OLLAMA_BASE_URL: {ollama_base_url}")
    
    if not use_ollama:
        print("\nâš ï¸  USE_OLLAMA æœªå•Ÿç”¨ï¼Œå°‡ä½¿ç”¨ OpenAI")
    else:
        print("\nâœ… Ollama å·²å•Ÿç”¨")
    
    return use_ollama

def test_llm_config():
    """æ¸¬è©¦ LLM é…ç½®"""
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ 2: LLM é…ç½®ç”Ÿæˆ")
    print("=" * 70)
    
    # æ¨¡æ“¬ get_llm_config é‚è¼¯
    use_ollama = os.getenv("USE_OLLAMA", "false").lower() == "true"
    
    if use_ollama:
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        ollama_model = os.getenv("OLLAMA_MODEL", "mistral")
        
        llm_config = {
            "config_list": [{
                "model": ollama_model,
                "api_key": "ollama",
                "base_url": ollama_base_url,
                "api_type": "openai",
            }],
            "temperature": 0.7,
            "timeout": 120,
            "max_tokens": 2048,
        }
        
        print("\nğŸ¦™ Ollama LLM é…ç½®:")
        print(f"  Model: {llm_config['config_list'][0]['model']}")
        print(f"  Base URL: {llm_config['config_list'][0]['base_url']}")
        print(f"  API Type: {llm_config['config_list'][0]['api_type']}")
        print(f"  Temperature: {llm_config['temperature']}")
        print(f"  Max Tokens: {llm_config['max_tokens']}")
    else:
        openai_api_key = os.getenv("OPENAI_API_KEY", "sk-...")
        openai_model = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
        
        llm_config = {
            "config_list": [{
                "model": openai_model,
                "api_key": openai_api_key[:10] + "..." if len(openai_api_key) > 10 else "***"
            }]
        }
        
        print("\nğŸ”‘ OpenAI LLM é…ç½®:")
        print(f"  Model: {llm_config['config_list'][0]['model']}")
        print(f"  API Key: {llm_config['config_list'][0]['api_key']}")
    
    return llm_config

def test_agent_initialization():
    """æ¸¬è©¦ Agent åˆå§‹åŒ–"""
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ 3: Agent åˆå§‹åŒ–æ¨¡æ“¬")
    print("=" * 70)
    
    use_ollama = os.getenv("USE_OLLAMA", "false").lower() == "true"
    
    # æ¨¡æ“¬ Agent åˆå§‹åŒ–
    agents_to_init = [
        "HostAgent",
        "SearchCaseAgent",
        "DeepAnalysisAgent",
        "SummaryAgent",
        "SearchLawAgent",
        "CustomizeConstraintAgent"
    ]
    
    print(f"\nä½¿ç”¨å¾Œç«¯: {'ğŸ¦™ Ollama' if use_ollama else 'ğŸ”‘ OpenAI'}\n")
    
    for agent_name in agents_to_init:
        status = "âœ… å¯ä»¥åˆå§‹åŒ–" if use_ollama else "âœ… å¯ä»¥åˆå§‹åŒ–"
        print(f"  [{agent_name}] {status}")
    
    print(f"\nâœ… æ‰€æœ‰ {len(agents_to_init)} å€‹ Agent éƒ½å°‡ä½¿ç”¨ç›¸åŒçš„ LLM é…ç½®")
    
    return True

def test_ollama_connectivity():
    """æ¸¬è©¦ Ollama æœå‹™é€£æ¥"""
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ 4: Ollama æœå‹™é€£æ¥")
    print("=" * 70)
    
    use_ollama = os.getenv("USE_OLLAMA", "false").lower() == "true"
    
    if not use_ollama:
        print("\nâ­ï¸  è·³éï¼šæœªå•Ÿç”¨ Ollama")
        return True
    
    ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    try:
        import requests
        
        print(f"\nğŸ” æª¢æŸ¥ Ollama æœå‹™: {ollama_base_url}")
        
        # æª¢æŸ¥æœå‹™æ˜¯å¦é‹è¡Œ
        response = requests.get(f"{ollama_base_url}/api/tags", timeout=5)
        
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Ollama æœå‹™å·²é€£æ¥")
            print(f"   å·²ä¸‹è¼‰çš„æ¨¡å‹æ•¸: {len(models)}")
            
            for model in models[:5]:
                model_name = model.get("name", "Unknown")
                print(f"   - {model_name}")
            
            if len(models) > 5:
                print(f"   ... ä»¥åŠ {len(models) - 5} å€‹å…¶ä»–æ¨¡å‹")
            
            return True
        else:
            print(f"âŒ é€£æ¥å¤±æ•—: HTTP {response.status_code}")
            return False
            
    except ImportError:
        print("\nâš ï¸  requests æ¨¡çµ„æœªå®‰è£ï¼Œè·³éé€£æ¥æª¢æŸ¥")
        print("   å®‰è£: pip install requests")
        return None
    except Exception as e:
        print(f"\nâŒ é€£æ¥å¤±æ•—: {e}")
        print(f"   ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ: ollama serve")
        return False

def test_agent_system_flow():
    """æ¸¬è©¦ Agent ç³»çµ±æµç¨‹"""
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ 5: Agent ç³»çµ±æµç¨‹æ¨¡æ“¬")
    print("=" * 70)
    
    use_ollama = os.getenv("USE_OLLAMA", "false").lower() == "true"
    backend = "ğŸ¦™ Ollama" if use_ollama else "ğŸ”‘ OpenAI"
    
    print(f"\nåŸ·è¡Œæµç¨‹ï¼ˆä½¿ç”¨ {backend}ï¼‰:\n")
    
    flow = [
        ("1. ç”¨æˆ¶è¼¸å…¥", "ç”¨æˆ¶åœ¨ UI ä¸­è¼¸å…¥æŸ¥è©¢"),
        ("2. ç²å–é…ç½®", "app.py::get_llm_config() è®€å–ç’°å¢ƒè®Šæ•¸"),
        ("3. åˆå§‹åŒ– Agents", "æ‰€æœ‰ Agent ä½¿ç”¨ç›¸åŒçš„ llm_config"),
        ("4. å•Ÿå‹•èŠå¤©", "ChatManager.start_chat() å•Ÿå‹•ç¾¤çµ„èŠå¤©"),
        ("5. Agent äº¤äº’", "å„ Agent èª¿ç”¨ LLMï¼ˆOllama æˆ– OpenAIï¼‰"),
        ("6. å·¥å…·èª¿ç”¨", "SearchCaseAgent, DeepAnalysisAgent ç­‰èª¿ç”¨å·¥å…·"),
        ("7. è¿”å›çµæœ", "æœ€çµ‚çµæœé€šé UI é¡¯ç¤º"),
    ]
    
    for step, description in flow:
        print(f"  {step}")
        print(f"    â””â”€ {description}")
    
    print(f"\nâœ… æ•´å€‹æµç¨‹æ”¯æŒ Ollama")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("\n")
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " " * 68 + "â•‘")
    print("â•‘" + "  ğŸ¦™ Agent System Ollama æ”¯æ´æ¸¬è©¦".center(68) + "â•‘")
    print("â•‘" + " " * 68 + "â•‘")
    print("â•š" + "â•" * 68 + "â•")
    print()
    
    try:
        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        use_ollama = test_environment_setup()
        llm_config = test_llm_config()
        test_agent_initialization()
        connectivity_status = test_ollama_connectivity()
        test_agent_system_flow()
        
        # ç¸½çµ
        print("\n" + "=" * 70)
        print("æ¸¬è©¦ç¸½çµ")
        print("=" * 70)
        
        backend = "ğŸ¦™ Ollama" if use_ollama else "ğŸ”‘ OpenAI"
        print(f"\nâœ… ä½¿ç”¨å¾Œç«¯: {backend}")
        
        if use_ollama and connectivity_status is False:
            print("âš ï¸  è­¦å‘Š: Ollama æœå‹™æœªé€£æ¥")
            print("   ç¢ºä¿: ollama serve æ­£åœ¨é‹è¡Œ")
        elif use_ollama and connectivity_status is True:
            print("âœ… Ollama æœå‹™å·²é€£æ¥")
        
        print("\nâœ… Agent System å·²æº–å‚™å°±ç·’ï¼\n")
        print("å•Ÿå‹•æ‡‰ç”¨:")
        print("  ./start_with_ollama.sh")
        print("  æˆ–")
        print("  export USE_OLLAMA=true")
        print("  chainlit run app.py\n")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
