import os
import json
import argparse
from typing import List, Dict, Any, Tuple
import logging
import re
import numpy as np
from rank_bm25 import BM25Okapi
# å‘é‡è³‡æ–™åº«
from chromadb import Client, Settings
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
from FlagEmbedding import FlagReranker
from dotenv import load_dotenv
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®‰å…¨åœ°è®€å–APIé‡‘é‘°
def get_openai_api_key():
    api_key = OPENAI_API_KEY
    return api_key

class LegalSearchEngine:
    def __init__(self, 
                 persist_directory: str = "./chroma_db",
                 collection_name: str = "law_articles_csv",
                 reranker_model: str = "BAAI/bge-reranker-v2-m3",
                 use_fp16: bool = False,
                 hybrid_alpha: float = 0.8):  # æ··åˆæœå°‹æ¬Šé‡
        """
        åˆå§‹åŒ–æ³•å¾‹æœå°‹å¼•æ“
        
        Args:
            persist_directory: Chromaè³‡æ–™åº«å­˜å„²ä½ç½®
            collection_name: é›†åˆåç¨±
            reranker_model: é‡æ’åºæ¨¡å‹åç¨±
            use_fp16: æ˜¯å¦ä½¿ç”¨FP16ç²¾åº¦
            hybrid_alpha: æ··åˆæœå°‹ä¸­å‘é‡æœå°‹çš„æ¬Šé‡ (0-1)ï¼ŒBM25çš„æ¬Šé‡ç‚º 1-hybrid_alpha
        """
        # åˆå§‹åŒ–Chromaå®¢æˆ¶ç«¯
        self.client = Client(Settings(
            persist_directory=persist_directory,
            is_persistent=True
        ))
        
        # æ··åˆæœå°‹åƒæ•¸
        self.hybrid_alpha = hybrid_alpha
        
        # åˆå§‹åŒ–OpenAIåµŒå…¥å‡½æ•¸
        self.embedding_function = OpenAIEmbeddingFunction(
            api_key=get_openai_api_key(),
            model_name="text-embedding-ada-002"
        )
        
        # ç²å–é›†åˆ
        try:
            self.collection = self.client.get_collection(
                collection_name, 
                embedding_function=self.embedding_function
            )
            logger.info(f"æˆåŠŸé€£æ¥åˆ°é›†åˆ: {collection_name}")
            
            # è®€å–æ‰€æœ‰æ–‡ä»¶ç”¨æ–¼BM25
            self.initialize_bm25()
            
        except Exception as e:
            logger.error(f"é€£æ¥é›†åˆå¤±æ•—: {str(e)}")
            raise
        
        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
        try:
            import torch
            if torch.cuda.is_available():
                device = "cuda"
                use_fp16 = True
                print(f"âœ… æª¢æ¸¬åˆ° CUDA GPU: {torch.cuda.get_device_name(0)}")
            elif torch.backends.mps.is_available():
                device = "mps"
                use_fp16 = False  # MPS ç›®å‰å° FP16 æ”¯æ´ä¸ç©©å®š
                print("ğŸ æª¢æ¸¬åˆ° Apple MPS GPUï¼ŒåŠ é€Ÿæ¨¡å¼å•Ÿç”¨")
            else:
                device = "cpu"
                use_fp16 = False
                print("âš ï¸ æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼")
            self.reranker = FlagReranker(reranker_model, use_fp16=use_fp16, device=device)
            logger.info(f"æˆåŠŸè¼‰å…¥é‡æ’åºæ¨¡å‹: {reranker_model} åˆ°è¨­å‚™: {device}")
        except Exception as e:
            logger.error(f"è¼‰å…¥é‡æ’åºæ¨¡å‹å¤±æ•—: {str(e)}")
            raise
    
    def initialize_bm25(self):
        """åˆå§‹åŒ–BM25æœå°‹ç´¢å¼•"""
        try:
            # ç²å–å…¨éƒ¨æ–‡æª”
            result = self.collection.get()
            
            if not result['documents']:
                logger.warning("æœªæ‰¾åˆ°æ–‡æª”ï¼Œç„¡æ³•åˆå§‹åŒ–BM25")
                return
                
            self.all_documents = result['documents']
            self.all_metadatas = result['metadatas']
            self.all_ids = result['ids']
            
            # ç‚ºBM25æº–å‚™æ–‡æª” - åˆ†è©
            tokenized_docs = []
            for doc in self.all_documents:
                # ç°¡å–®çš„åˆ†è©ï¼Œå¯ä¾éœ€æ±‚èª¿æ•´
                tokens = self.tokenize(doc)
                tokenized_docs.append(tokens)
                
            # å‰µå»ºBM25ç´¢å¼•
            self.bm25 = BM25Okapi(tokenized_docs)
            logger.info(f"æˆåŠŸåˆå§‹åŒ–BM25ç´¢å¼•ï¼Œå…± {len(tokenized_docs)} å€‹æ–‡æª”")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–BM25ç´¢å¼•å¤±æ•—: {str(e)}")
            # è¨­ç½®ç‚ºNoneï¼Œå¾ŒçºŒä½¿ç”¨æ™‚å†æª¢æŸ¥
            self.bm25 = None
    
    def tokenize(self, text: str) -> List[str]:
        """ç°¡å–®çš„ä¸­æ–‡åˆ†è©è™•ç†"""
        # æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤æ¨™é»ç¬¦è™Ÿï¼‰
        text = re.sub(r'[^\w\s]', ' ', text)
        # é‡å°ä¸­æ–‡ç‰¹æ€§ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨å­—ç¬¦ç´šåˆ†è©
        # é€™è£¡ä½¿ç”¨ç°¡å–®çš„ç©ºæ ¼åˆ†è©ï¼Œå¯¦éš›æ‡‰ç”¨å¯èƒ½éœ€è¦çµå·´åˆ†è©ç­‰å·¥å…·
        tokens = text.split()
        # ç¢ºä¿å­—ç¬¦ç´šåˆ‡åˆ†
        char_tokens = []
        for token in tokens:
            if len(token) > 1:  # å°æ–¼å¤šå­—ç¬¦è©ï¼Œæ‹†åˆ†æˆå–®å­—ç¬¦
                for char in token:
                    char_tokens.append(char)
            else:
                char_tokens.append(token)
        return char_tokens
    
    def bm25_search(self, query: str, top_k: int = 15) -> List[Dict[str, Any]]:
        """ä½¿ç”¨BM25é€²è¡Œæœå°‹"""
        if self.bm25 is None:
            logger.warning("BM25ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œåƒ…ä½¿ç”¨å‘é‡æœå°‹")
            return []
            
        # æŸ¥è©¢åˆ†è©
        query_tokens = self.tokenize(query)
        
        # BM25æœå°‹
        bm25_scores = self.bm25.get_scores(query_tokens)
        
        # å°åˆ†æ•¸é€²è¡Œæ’åº
        top_indices = np.argsort(bm25_scores)[::-1][:top_k]
        
        # æ”¶é›†çµæœ
        results = []
        for idx in top_indices:
            if bm25_scores[idx] > 0:  # åƒ…è¿”å›æœ‰åŒ¹é…çš„çµæœ
                results.append({
                    "id": self.all_ids[idx],
                    "content": self.all_documents[idx],
                    "metadata": self.all_metadatas[idx],
                    "score": float(bm25_scores[idx]),  # numpy floatè½‰ç‚ºPython float
                    "source": "bm25"
                })
                
        logger.info(f"BM25æœå°‹æ‰¾åˆ° {len(results)} å€‹çµæœ")
        return results

    def vector_search(self, query: str, top_k: int = 15) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å‘é‡æœå°‹"""
        try:
            search_results = self.collection.query(
                query_texts=[query],
                n_results=top_k
            )
            
            documents = search_results['documents'][0]
            metadatas = search_results['metadatas'][0]
            ids = search_results['ids'][0]
            
            if not documents:
                logger.warning("æœªæ‰¾åˆ°ç›¸é—œçµæœ")
                return []
                
            logger.info(f"å‘é‡æœå°‹æ‰¾åˆ° {len(documents)} å€‹çµæœ")
            
            results = []
            for i in range(len(documents)):
                results.append({
                    "id": ids[i],
                    "content": documents[i],
                    "metadata": metadatas[i],
                    "score": 1.0,  # åˆå§‹åˆ†æ•¸ï¼Œç¨å¾Œæœƒè¢«é‡æ–°æ’åº
                    "source": "vector"
                })
                
            return results
            
        except Exception as e:
            logger.error(f"å‘é‡æœå°‹éç¨‹å‡ºéŒ¯: {str(e)}")
            return []

    def search(self, query: str, top_k: int = 15, rerank_top_n: int = 5) -> List[Dict[str, Any]]:
        """
        æ··åˆæœå°‹çµåˆBM25èˆ‡å‘é‡æœå°‹
        """
        logger.info(f"åŸ·è¡Œæ··åˆæœå°‹: {query}")
        
        # åŸ·è¡ŒBM25æœå°‹
        bm25_results = self.bm25_search(query, top_k)
        
        # åŸ·è¡Œå‘é‡æœå°‹
        vector_results = self.vector_search(query, top_k)
        
        # åˆä½µçµæœ
        combined_results = {}
        
        # æ·»åŠ BM25çµæœï¼Œæ­¸ä¸€åŒ–åˆ†æ•¸
        if bm25_results:
            max_bm25_score = max([r["score"] for r in bm25_results]) if bm25_results else 1.0
            for r in bm25_results:
                doc_id = r["id"]
                norm_score = r["score"] / max_bm25_score
                combined_results[doc_id] = {
                    "id": doc_id,
                    "content": r["content"],
                    "metadata": r["metadata"],
                    "bm25_score": norm_score,
                    "vector_score": 0.0,
                    "sources": ["bm25"]
                }
        
        # æ·»åŠ å‘é‡æœå°‹çµæœ
        for r in vector_results:
            doc_id = r["id"]
            if doc_id in combined_results:
                combined_results[doc_id]["sources"].append("vector")
                combined_results[doc_id]["vector_score"] = 1.0  # æš«æ™‚è¨­ç‚º1.0ï¼Œå¾ŒçºŒæœƒé‡æ–°æ’åº
            else:
                combined_results[doc_id] = {
                    "id": doc_id,
                    "content": r["content"],
                    "metadata": r["metadata"],
                    "bm25_score": 0.0,
                    "vector_score": 1.0,  # æš«æ™‚è¨­ç‚º1.0ï¼Œå¾ŒçºŒæœƒé‡æ–°æ’åº
                    "sources": ["vector"]
                }
        
        # è½‰æ›ç‚ºåˆ—è¡¨
        all_results = list(combined_results.values())
        
        # ä½¿ç”¨é‡æ’åºæ¨¡å‹é‡æ–°æ’åº
        if all_results:
            try:
                # ç²å–æ–‡æª”å…§å®¹
                docs = [r["content"] for r in all_results]
                
                # ä½¿ç”¨é‡æ’åºæ¨¡å‹è¨ˆç®—ç›¸é—œæ€§å¾—åˆ†
                ranking_scores = []
                for doc in docs:
                    score = self.reranker.compute_score([query, doc])
                    if isinstance(score, list):
                        score = score[0] if score else 0.0
                    ranking_scores.append(score)
                
                # æ›´æ–°å‘é‡åˆ†æ•¸
                for i, score in enumerate(ranking_scores):
                    all_results[i]["vector_score"] = score
                
                # è¨ˆç®—æ··åˆå¾—åˆ†
                for r in all_results:
                    r["final_score"] = self.hybrid_alpha * r["vector_score"] + \
                                      (1 - self.hybrid_alpha) * r["bm25_score"]
                
                # æŒ‰æ··åˆåˆ†æ•¸æ’åº
                all_results.sort(key=lambda x: x["final_score"], reverse=True)
                
                # æˆªå–å‰Nå€‹çµæœ
                all_results = all_results[:rerank_top_n]
                
                # ç‚ºè¿”å›çµæœæ¨™æº–åŒ–
                for i, r in enumerate(all_results):
                    r["rank"] = i + 1
                    r["score"] = r["final_score"]  # ä½¿ç”¨æœ€çµ‚å¾—åˆ†ä½œç‚ºscore
                
                logger.info(f"å®Œæˆæ··åˆæœå°‹èˆ‡é‡æ’åºï¼Œè¿”å›å‰ {len(all_results)} å€‹çµæœ")
                
            except Exception as e:
                logger.error(f"é‡æ’åºéç¨‹å‡ºéŒ¯: {str(e)}")
                # ç°¡å–®åœ°æŒ‰BM25èˆ‡å‘é‡æœå°‹ä¾†æºæ’åº
                for r in all_results:
                    sources = r["sources"]
                    r["score"] = 1.0 if "bm25" in sources and "vector" in sources else \
                                0.8 if "vector" in sources else \
                                0.6 if "bm25" in sources else 0.0
                
                # æ’åºä¸¦æˆªå–
                all_results.sort(key=lambda x: x["score"], reverse=True)
                all_results = all_results[:rerank_top_n]
        
        return all_results
    
    def get_related_laws(self, query: str, top_k: int = 10, rerank_top_n: int = 5) -> Tuple[List[Dict], List[Dict]]:
        results = self.search(query, top_k, rerank_top_n)
        
        # å€åˆ†ç›´æ¥ç›¸é—œå’Œé–“æ¥ç›¸é—œæ³•æ¢
        direct_relevant = []
        indirectly_relevant = []
        
        for result in results:
            score = result["score"]
            if isinstance(score, list):
                score = score[0] if score else 0.0
            
            # å¾—åˆ†é«˜æ–¼é–¾å€¼çš„è¦–ç‚ºç›´æ¥ç›¸é—œ
            if score > 0.75:
                direct_relevant.append(result)
            else:
                indirectly_relevant.append(result)
        
        return direct_relevant, indirectly_relevant

    def format_results(self, results: List[Dict]) -> str:
        """æ ¼å¼åŒ–æœå°‹çµæœç‚ºæ˜“è®€çš„æ–‡å­—"""
        formatted = []
        
        for i, result in enumerate(results):
            metadata = result["metadata"]
            law_name = metadata.get("æ³•å¾‹åç¨±", "æœªçŸ¥æ³•å¾‹")
            article = metadata.get("æ¢", "æœªçŸ¥æ¢æ¬¾")
            
            # ç²å–æœå°‹ä¾†æºä¿¡æ¯
            sources = result.get("sources", [])
            source_info = f"[ä¾†æº: {', '.join(sources)}]" if sources else ""
            
            formatted.append(f"çµæœ {i+1} [ç›¸é—œåº¦: {result['score']:.2f}] {source_info}")
            formatted.append(f"æ³•å¾‹: {law_name} ç¬¬ {article} æ¢")
            formatted.append(f"å…§å®¹:\n{result['content']}\n")
            formatted.append("-" * 80)
        
        return "\n".join(formatted)


def main():
    parser = argparse.ArgumentParser(description='æ³•å¾‹æ–‡æœ¬å‘é‡æœå°‹å·¥å…·')
    parser.add_argument('query', nargs='?', default='', help='æœå°‹æŸ¥è©¢')
    parser.add_argument('--top-k', type=int, default=15, help='åˆå§‹æª¢ç´¢çš„çµæœæ•¸é‡')
    parser.add_argument('--rerank-top-n', type=int, default=5, help='é‡æ’åºå¾Œè¿”å›çš„çµæœæ•¸é‡')
    parser.add_argument('--collection', default='law_articles', help='é›†åˆåç¨±')
    parser.add_argument('--persist-dir', default='./chroma_db', help='Chromaè³‡æ–™åº«ç›®éŒ„')
    parser.add_argument('--output', help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æœå°‹å¼•æ“
    try:
        search_engine = LegalSearchEngine(
            persist_directory=args.persist_dir,
            collection_name=args.collection
        )
        
        # å¦‚æœæ²’æœ‰æä¾›æŸ¥è©¢ï¼Œå‰‡é€²å…¥äº’å‹•æ¨¡å¼
        if not args.query:
            print("æ­¡è¿ä½¿ç”¨æ³•å¾‹æ–‡æœ¬å‘é‡æœå°‹å·¥å…·")
            print("è«‹è¼¸å…¥æ‚¨çš„æŸ¥è©¢ (è¼¸å…¥ 'q' é€€å‡º):")
            
            while True:
                query = input("> ")
                if query.lower() == 'q':
                    break
                    
                direct_relevant, indirectly_relevant = search_engine.get_related_laws(
                    query, args.top_k, args.rerank_top_n
                )
                
                print("\nç›´æ¥ç›¸é—œæ³•æ¢:")
                print(search_engine.format_results(direct_relevant))
                
                if indirectly_relevant:
                    print("\né–“æ¥ç›¸é—œæ³•æ¢:")
                    print(search_engine.format_results(indirectly_relevant))
                
                print("\nè«‹è¼¸å…¥æ–°çš„æŸ¥è©¢ (è¼¸å…¥ 'q' é€€å‡º):")
        else:
            # ç›´æ¥åŸ·è¡Œå–®æ¬¡æŸ¥è©¢
            direct_relevant, indirectly_relevant = search_engine.get_related_laws(
                args.query, args.top_k, args.rerank_top_n
            )
            
            results = {
                "query": args.query,
                "direct_relevant": direct_relevant,
                "indirectly_relevant": indirectly_relevant
            }
            
            # è¼¸å‡ºçµæœ
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"çµæœå·²ä¿å­˜è‡³ {args.output}")
            else:
                print("\nç›´æ¥ç›¸é—œæ³•æ¢:")
                print(search_engine.format_results(direct_relevant))
                
                if indirectly_relevant:
                    print("\né–“æ¥ç›¸é—œæ³•æ¢:")
                    print(search_engine.format_results(indirectly_relevant))
    
    except Exception as e:
        logger.error(f"åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {str(e)}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())